{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrZa6-kNYoBL",
        "colab_type": "text"
      },
      "source": [
        "Text Preprocessing:\n",
        "===\n",
        "In this notebook, you will discover what is text preprocessing and how to pre-process text before extracting features from it or feeding it into a machine learning model.\n",
        "\n",
        "\n",
        "*You can read the full blog at [here](https://gauravchopracg.github.io/Text-Preprocessing/).*\n",
        "\n",
        "\n",
        "Text preprocessing is the process of cleaning the data, preparing the data to be used for machine learning. The text preprocessing steps consider in this notebook are:\n",
        "\n",
        "1. Tokenization\n",
        "2. Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFBIwufvnwWt",
        "colab_type": "text"
      },
      "source": [
        "Tokenization:\n",
        "---\n",
        "Tokenization is the process of splitting an input text into meaningful chunks. We can think of text as a sequence of words and further word as a meaningful sequence of characters (We can also think text as a sequence of characters or phrases but right now we will consider words as a part of text for simpler understanding). In that way, we need to first split text into small chunks which we will call tokens, a token is a useful unit for further semantic processing. \n",
        "\n",
        "We can split a text \n",
        "* by whitespaces,\n",
        "* by punctuation, or\n",
        "* any set of rules specifically to that task, in this notebook we will consider the splitting of text by the rules of English grammar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "312vCM3bnwW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.608310Z",
          "start_time": "2017-11-05T18:16:26.423528Z"
        },
        "id": "uItaK59cnwXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"This is Andrew's text, isn't it?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoMzdZLjkXX-",
        "colab_type": "text"
      },
      "source": [
        "### Split by white spaces:\n",
        "\n",
        "To split the text into tokens or meaningful words using white spaces, we can use python library NLTK, it offers different classes of tokenizer which we can use it to split text into meaningful chunks for example to splits the input sequence on white spaces, that could be a space or any other character that is not visible. We can use nltk.tokenize.WhitespaceTokenizer() function to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.633134Z",
          "start_time": "2017-11-05T18:16:27.610910Z"
        },
        "id": "WfD2Q849nwXq",
        "colab_type": "code",
        "outputId": "2f5fe910-5109-4edf-8b02-84a5dfddb37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s6N9W1ekmFa",
        "colab_type": "text"
      },
      "source": [
        "However, the problem is 'text,' and 'text' are two different words for tokenizer similarly 'it' and 'it?' we might want to merge these two tokens because they have essentially the same meaning,."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npunaeZWk2lt",
        "colab_type": "text"
      },
      "source": [
        "### Split by punctuation\n",
        "\n",
        "Similarly as before, we can split the text by punctuation using nltk.tokenize.WordPunctTokenizer() and the result will be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.660827Z",
          "start_time": "2017-11-05T18:16:27.651961Z"
        },
        "id": "4mzCPV1bnwYQ",
        "colab_type": "code",
        "outputId": "be484f79-258d-4e49-d5d6-119abc6ca9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m1324P_k9gC",
        "colab_type": "text"
      },
      "source": [
        "the problem is 's', 'isn' 't' are not very meaningful and punctuation are different tokens hence, it doesn't make sense to analyze them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkm58OUNlDG_",
        "colab_type": "text"
      },
      "source": [
        "### Split by set of heuristics\n",
        "\n",
        "We can also come up with a set of rules or heuristics which can be easily found in TreebankWordTokenizer and it actually uses rules of english language grammar to make it tokenization that actually makes sense for further analysis. In reality this is very close to perfect tokenization that we want for English language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.647746Z",
          "start_time": "2017-11-05T18:16:27.637909Z"
        },
        "id": "KwizBixMnwX9",
        "colab_type": "code",
        "outputId": "3987f1af-32f6-4d34-860b-37c20d5edd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPgKZ_kWlPdG",
        "colab_type": "text"
      },
      "source": [
        "Text Normalization:\n",
        "---\n",
        "The next thing we might want to do is token normalization. We may want the same token for different forms of the word. like wolf, wolves -> wolf or talk, talks -> talk. The process of normalizing the words into same form is called Text Normalization. They consist of:\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "\n",
        "We will first define a sample text, tokenize it and then experiment with different text normalization techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.674332Z",
          "start_time": "2017-11-05T18:16:27.666509Z"
        },
        "id": "rr7WbZ-znwZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"feet wolves cats talked\"\n",
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_lKxeWLnwYw",
        "colab_type": "text"
      },
      "source": [
        "### Stemming:\n",
        "\n",
        "Stemming is the process of removing and replacing suffixes to get to the root form of the word, which is called the stem. It is usually refers to heuristics that chop off suffixes. To apply stemming, we can use NLTK library function nltk.stem.PorterStemmer(), it is the oldest stemmer for English language. It has five heuristic phases of word reductions applied sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:27.693761Z",
          "start_time": "2017-11-05T18:16:27.677877Z"
        },
        "id": "ArMph-tVnwZt",
        "colab_type": "code",
        "outputId": "683c3399-dd67-47d8-c2fb-213f4d516f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\" \".join(stemmer.stem(token) for token in tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'feet wolv cat talk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeSrq4Tkl5oW",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "Lemmatization is usually refers to doing things properly with the use of a vocabulary and morphological analysis. It returns the base or dictionary form of a word, which is known as the lemma. To apply lemmatization, we can use NLTK library function nltk.stem.WordNetLemmatizer(), it uses WordNet Database to lookup lemmas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-05T18:16:30.840117Z",
          "start_time": "2017-11-05T18:16:27.698683Z"
        },
        "id": "ejIsQHhTnwaS",
        "colab_type": "code",
        "outputId": "884801b1-7219-4e23-9433-73ddd07efd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "stemmer = nltk.stem.WordNetLemmatizer()\n",
        "\" \".join(stemmer.lemmatize(token) for token in tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'foot wolf cat talked'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDi24f3omGLB",
        "colab_type": "text"
      },
      "source": [
        "In reality, we need to try both stemming or lemmatization to decide which is best for our task"
      ]
    }
  ]
}